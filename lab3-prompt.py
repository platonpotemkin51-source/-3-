import pandas as pd
import numpy as np
from openai import OpenAI
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_gigachat.embeddings.gigachat import GigaChatEmbeddings
from langchain_gigachat.chat_models import GigaChat
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
import ast


# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
def load_embeddings_to_dataframe(filepath):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ CSV —Ñ–∞–π–ª–∞ –≤ pandas DataFrame

    Args:
        filepath (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏

    Returns:
        pd.DataFrame: DataFrame —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    try:
        df = pd.read_csv(filepath)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ñ–∞–π–ª–∞ {filepath}")
        print(f"–ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}")
        print(f"\n–ü–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏:")
        print(df.head(3))
        return df
    except Exception as e:
        print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None


# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = load_embeddings_to_dataframe("arxiv_embeddings202505211515.csv")

# –ü—Ä–æ—Å–º–æ—Ç—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
print(f"\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n{df.dtypes}")
print(f"\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:\n{df.isnull().sum()}")


def prepare_documents(df, limit=1000):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç DataFrame –≤ —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ LangChain

    Args:
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç–∞—Ç–µ–π
        limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

    Returns:
        List[Document]: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    documents = []

    # –í–æ–∑—å–º–∏—Ç–µ –ø–µ—Ä–≤—ã–µ limit –∑–∞–ø–∏—Å–µ–π
    df_subset = df.head(limit)

    for idx, row in df_subset.iterrows():
        # –§–æ—Ä–º–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–æ–ª–µ–π
        page_content = ''

        # –ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        if 'title' in row:
            page_content += f"–ù–∞–∑–≤–∞–Ω–∏–µ: {row['title']}\n"
        if 'abstract' in row:
            page_content += f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è: {row['abstract']}\n"
        if 'authors' in row:
            page_content += f"–ê–≤—Ç–æ—Ä—ã: {row['authors']}\n"

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {}
        if 'categories' in row:
            metadata['categories'] = row['categories']
        if 'main_category' in row:
            metadata['main_category'] = row['main_category']
        if 'year' in row:
            metadata['year'] = row['year']
        if 'article_id' in row:
            metadata['article_id'] = row['article_id']

        documents.append(Document(
            page_content=page_content,
            metadata=metadata
        ))

    print(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    return documents


# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
documents = prepare_documents(df)

# –ü—Ä–æ—Å–º–æ—Ç—Ä –ø—Ä–∏–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
print("–ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
print(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {documents[0].page_content[:200]}...")
print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {documents[0].metadata}")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
client = OpenAI(
    api_key="ZmJhMjUwZTItMDg0ZC00N2E3LWIyNDktYjA4MTQyZGFmMGE4.97f6d089a16317c3aa93b365eda739a8",
    base_url="https://foundation-models.api.cloud.ru/v1"
)


def get_embedding(text: str, model="BAAI/bge-m3") -> list:
    """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞"""
    response = client.embeddings.create(
        input=[text],
        model=model
    )
    return response.data[0].embedding


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
test_embedding = get_embedding("–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(test_embedding)}")

from langchain_core.embeddings import Embeddings
from typing import List


class CustomEmbeddings(Embeddings):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π –∫–ª–∞—Å—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API"""

    def __init__(self, client, model="BAAI/bge-m3"):
        self.client = client
        self.model = model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        embeddings = []
        for text in texts:
            response = self.client.embeddings.create(
                input=[text],
                model=self.model
            )
            embeddings.append(response.data[0].embedding)
        return embeddings

    def embed_query(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        response = self.client.embeddings.create(
            input=[text],
            model=self.model
        )
        return response.data[0].embedding


# –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
embeddings = CustomEmbeddings(client)

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ ChromaDB
# print("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
# vectorstore = Chroma.from_documents(
#     documents=documents,
#     embedding=embeddings,
#     collection_name="arxiv_papers",
#     persist_directory="./chroma_db"  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
# )
# print("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–æ!")


# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
# test_query = "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"
# results = vectorstore.similarity_search(test_query, k=3)
# print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{test_query}':")
# for i, doc in enumerate(results, 1):
#     print(f"\n{i}. {doc.page_content[:200]}...")
#     print(f"   –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {doc.metadata}")

# –ó–∞–¥–∞–Ω–∏–µ 1
# 1. –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å—Ç–∞—Ç–µ–π
# print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å—Ç–∞—Ç–µ–π:")
# category_stats = df['categories'].value_counts()
# print(category_stats)

# 2. –í—ã–±–æ—Ä –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∏–∑ 1000 —Å—Ç–∞—Ç–µ–π
# subset_df = df.sample(n=1000, random_state=33)
# print(f"–†–∞–∑–º–µ—Ä –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞: {subset_df.shape}")

# –ó–∞–¥–∞–Ω–∏–µ 2
# 1. –†–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ k
# print("1. –†–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è k:")
# for k in [1, 3, 5, 10]:
#     results = vectorstore.similarity_search(test_query, k=k)
#     print(f"k={k}: –Ω–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

# 2. –ü–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º
# print("2. –ü–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º:")
# queries = [
#     "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
#     "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞",
#     "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ",
#     "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"
# ]
#
# for query in queries:
#     results = vectorstore.similarity_search(query, k=2)
#     print(f"–ó–∞–ø—Ä–æ—Å: '{query}'")
#     print(f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(results)}")
#     for i, doc in enumerate(results, 1):
#         print(f"  {i}. {doc.page_content[:100]}...")

# 3. –ú–µ—Ç–æ–¥ similarity_search_with_score()
# print("3. –ü–æ–∏—Å–∫ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏:")
# results_with_scores = vectorstore.similarity_search_with_score(test_query, k=3)
# print(f"–ó–∞–ø—Ä–æ—Å: '{test_query}'")
# for i, (doc, score) in enumerate(results_with_scores, 1):
#     print(f"\n{i}. –û—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏: {score:.4f}")
#     print(f"   –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {doc.page_content[:150]}...")
#     print(f"   –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {doc.metadata}")

print("–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings,
    collection_name="arxiv_papers"
)
print("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ!")

# –ó–∞–¥–∞–Ω–∏–µ 3
# –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ —Å –ø–æ–∏—Å–∫–æ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}  # –í–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ç–æ–ø-5 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
)

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
query = "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
retrieved_docs = retriever.invoke(query)

print(f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(retrieved_docs)}")
for i, doc in enumerate(retrieved_docs, 1):
    print(f"\n–î–æ–∫—É–º–µ–Ω—Ç {i}:")
    print(doc.page_content[:150] + "...")

# MMR –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –º–µ–∂–¥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
retriever_mmr = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
        "lambda_mult": 0.5  # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é (1.0) –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º (0.0)
    }
)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
query = "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞"
docs_similarity = retriever.invoke(query)
docs_mmr = retriever_mmr.invoke(query)

print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞:")
print("\n=== Similarity Search ===")
for i, doc in enumerate(docs_similarity[:3], 1):
    print(f"{i}. {doc.page_content[:100]}...")

print("\n=== MMR Search ===")
for i, doc in enumerate(docs_mmr[:3], 1):
    print(f"{i}. {doc.page_content[:100]}...")

# –†–µ—Ç—Ä–∏–≤–µ—Ä —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –æ—Ü–µ–Ω–∫–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
retriever_threshold = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.7,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
        "k": 10
    }
)

# –í–ê–ñ–ù–û: –ù–µ –≤—Å–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç score_threshold
# –í —Å–ª—É—á–∞–µ ChromaDB –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –¥—Ä—É–≥–æ–π –ø–æ–¥—Ö–æ–¥

# 1. –ü–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º lambda_mult –≤ MMR
print("1. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å lambda_mult –≤ MMR:")

for lambda_val in [0.0, 0.5, 1.0]:
    retriever_mmr_test = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 3,
            "fetch_k": 50,
            "lambda_mult": lambda_val
        }
    )
    docs = retriever_mmr_test.invoke("–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏")
    print(f"lambda_mult={lambda_val}: –Ω–∞–π–¥–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    for i, doc in enumerate(docs, 1):
        content = doc.page_content
        metadata = doc.metadata

        # –í—ã—Ç–∞—â–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title = "–ù–µ –Ω–∞–π–¥–µ–Ω"
        if "–ù–∞–∑–≤–∞–Ω–∏–µ:" in content:
            title = content.split("–ù–∞–∑–≤–∞–Ω–∏–µ:")[1].split("\n")[0].strip()

        # –í—ã—Ç–∞—â–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        category = metadata.get('categories', '–ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')

        print(f"   –î–æ–∫—É–º–µ–Ω—Ç {i}:")
        print(f"      –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title[:80]}...")
        print(f"      –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")

# 2. –°–æ–∑–¥–∞–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ —Ä–∞–±–æ—Ç—ã —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
print("\n2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —Ä–∞–±–æ—Ç—ã:")

import time


def compare_search_times(query, vectorstore):
    search_types = {
        "similarity": {"search_type": "similarity", "search_kwargs": {"k": 5}},
        "mmr": {"search_type": "mmr", "search_kwargs": {"k": 5, "fetch_k": 20, "lambda_mult": 0.5}},
    }

    for name, config in search_types.items():
        retriever = vectorstore.as_retriever(**config)
        start_time = time.time()
        docs = retriever.invoke(query)
        end_time = time.time()
        print(f"{name}: {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞ {end_time - start_time:.3f} —Å–µ–∫")


compare_search_times("–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", vectorstore)

# –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ)

llm = GigaChat(
    credentials="OThhZGViNTgtN2E0Mi00YmExLTgzMTctM2YwNjFmNGI0NzNkOmM2YzYzMGJlLTczMGQtNDk3MC04MjRlLWQwZjBkZWRkM2U5Mg==",
    scope="GIGACHAT_API_B2B",
    model="GigaChat-Pro",
    verify_ssl_certs=False,
    timeout=30
)

# –¢–µ—Å—Ç —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
test_response = llm.invoke("–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ: —á—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?")
print(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {test_response.content}")

# –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è RAG
prompt_template = """–¢—ã -- –Ω–∞—É—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ -- –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –¢–û–õ–¨–ö–û –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π ArXiv.

–ü—Ä–∞–≤–∏–ª–∞:
1. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∏–∂–µ
2. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
3. –£–∫–∞–∑—ã–≤–∞–π, –∏–∑ –∫–∞–∫–∏—Ö —Å—Ç–∞—Ç–µ–π –≤–∑—è—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
4. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —á–µ—Ç–∫–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ
5. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –∫–∞—Å–∞–µ—Ç—Å—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π, –±—É–¥—å —Ç–æ—á–Ω—ã–º

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π:
{context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–û—Ç–≤–µ—Ç:"""

prompt = ChatPromptTemplate.from_template(prompt_template)


def format_docs(docs):
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –µ–¥–∏–Ω—É—é —Å—Ç—Ä–æ–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

    Args:
        docs: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ Document

    Returns:
        str: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    """
    context_parts = []
    for i, doc in enumerate(docs, 1):
        context_parts.append(f"[–î–æ–∫—É–º–µ–Ω—Ç {i}]")
        context_parts.append(doc.page_content)
        if doc.metadata:
            context_parts.append(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {doc.metadata}")
        context_parts.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    return "\n".join(context_parts)


# –¢–µ—Å—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
test_docs = retriever.invoke("–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏")
formatted_context = format_docs(test_docs[:2])
print("–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:")
print(formatted_context[:500] + "...")
# –°–æ–∑–¥–∞–Ω–∏–µ RAG-—Ü–µ–ø–æ—á–∫–∏
rag_chain = (
    {
        "context": retriever | format_docs,  # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        "question": RunnablePassthrough()     # –ü–µ—Ä–µ–¥–∞–µ–º –≤–æ–ø—Ä–æ—Å –∫–∞–∫ –µ—Å—Ç—å
    }
    | prompt      # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
    | llm         # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å
)

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG-—Å–∏—Å—Ç–µ–º—ã
questions = [
    "–ö–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π?",
    "–†–∞—Å—Å–∫–∞–∂–∏ –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞",
    "–ö–∞–∫–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø–æ–¥—Ö–æ–¥—ã –∫ –æ–±—É—á–µ–Ω–∏—é –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π?"
]

print("=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG-—Å–∏—Å—Ç–µ–º—ã ===\n")
for i, question in enumerate(questions, 1):
    print(f"–í–æ–ø—Ä–æ—Å {i}: {question}")
    print("-" * 80)

    try:
        response = rag_chain.invoke(question)
        print(f"–û—Ç–≤–µ—Ç: {response.content}\n")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}\n")


def interactive_rag_qa():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤"""
    print("=== –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π ArXiv ===")
    print("–í–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è\n")

    while True:
        question = input("–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()

        if question.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
            print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break

        if not question:
            continue

        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            docs = retriever.invoke(question)
            print(f"\nüìö –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = rag_chain.invoke(question)
            print(f"\nü§ñ –û—Ç–≤–µ—Ç:\n{response.content}\n")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            show_sources = input("–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏? (–¥–∞/–Ω–µ—Ç): ").strip().lower()
            if show_sources in ['–¥–∞', 'yes', 'y', '–¥']:
                print("\nüìñ –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
                for i, doc in enumerate(docs[:3], 1):
                    print(f"\n{i}. {doc.page_content[:200]}...")
                    print(f"   –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {doc.metadata}")

            print("\n" + "=" * 80 + "\n")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}\n")


print("–®–∞–≥ 4.6")

# –í—Å–µ–≥–æ 2 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞ –≤–º–µ—Å—Ç–æ 5
diverse_questions = [
    "–ß—Ç–æ —Ç–∞–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è?",
    "–ö–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏?"
]

print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–µ–º–∞—Ç–∏–∫–∞—Ö:\n")
for i, question in enumerate(diverse_questions, 1):
    print(f"–í–æ–ø—Ä–æ—Å {i}: {question}")
    print("-" * 60)

    try:
        response = rag_chain.invoke(question)
        print(f"–û—Ç–≤–µ—Ç: {response.content}\n")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}\n")

# –û–¥–∏–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å —Ä–∞–∑–Ω—ã–º k
print("–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å —Ä–∞–∑–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
for k in [3, 6]:  # –≤—Å–µ–≥–æ 2 –∑–Ω–∞—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ 3
    custom_retriever = vectorstore.as_retriever(search_kwargs={"k": k})
    custom_rag_chain = (
            {"context": custom_retriever | format_docs, "question": RunnablePassthrough()}
            | prompt | llm
    )

    response = custom_rag_chain.invoke("–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏")
    print(f"k={k}: –æ—Ç–≤–µ—Ç {len(response.content)} —Å–∏–º–≤–æ–ª–æ–≤")

# –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
interactive_rag_qa()
